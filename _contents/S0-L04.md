---
layout: post
title: GenAI Guardrails 
lecture: 
lectureVersion: next
extraContent: 
notes: team-2
video: team-3
tags:
- Protect
---

In this session, our readings cover: 

## Required Readings: 

### Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations
  + https://arxiv.org/abs/2312.06674
  + We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.
  


## More Readings: 


### Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection
+ [Submitted on 23 Feb 2023 (v1), last revised 5 May 2023 (this version, v2)]
+ https://arxiv.org/abs/2302.12173
+ Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz
+ Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.
+ Subjects: Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)

### A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly

+ [Submitted on 4 Dec 2023]
+ Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes findings into "The Good" (beneficial LLM applications), "The Bad" (offensive applications), and "The Ugly" (vulnerabilities and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code and data security, outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.

### Baseline Defenses for Adversarial Attacks Against Aligned Language Models
+ https://github.com/neelsjain/baseline-defenses 

<br /><br /><br />
## Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations
(Page 10-22)
### Experiments
+ Due to the lack of standardized taxonomies, different models were trained on and tested on different datasets all with their own taxonomy.
+ Llama Guard is evaluated on two axes:
    1. In-domain performance on its **own datasets and taxonomy** (on-policy setting)
    2. Adaptability to **other taxonomies** (off-policy setting)
#### Evaluation Methodology
+ To evaluate on several datasets, all with different taxonomies, different bars and without clear mapping, there are three techniques used to subjectively evaluate the models.
    1. **Overall binary classification for APIs that provide per-category output**: this method assigns positive label if any positive label is predicted, regardless of whether it aligns with GT target category.
        * ex: text1 -> Predicted: violence&hate GT: sexual content Result: unsafe, right prediction
    2. **Per-category binary classification via 1-vs-all**: returns unsafe only if the input violates target category. *This method focuses on models' ability to **predict the category right**, rather than differentiate safe and unsafe.*
        * ex: text2 -> violence&hate GT: sexual context Result: safe, wrong prediction 
        * ex: text2 -> violence&hate GT: violence&hate Result: unsafe, right prediction 
    3. **Per-category binary classification via 1-vs-benign**: only benign labels are considered negative, removes hard negatives. *If a positively labeled sample belonging to a category that is not the target category, it will be dropped*
        * ex: calculating precision=TP/(TP+FP), less likely to predict false positive as less actual negative exists
 + The second method is used for evaluating Llama Guard both for the internal test set and for other datasets
 + The authors follow the third approach for all the baseline APIs that they evaluate.
 
#### Benchmarks and Baselines
+ **Benchmarks (datasets):**
    1. ToxicChat: 10k, real-world user-AI interactions.
    2. OpenAI Moderation Evaluation Dataset: 1,680 prompt examples, labeled according the OpenAI moderation API taxonomy
+ **Baselines (models):**
    1. OpenAI Moderation API: GPT-based, multi-label fine-tuned classifier 
    2. Perspective API: for online platforms and publishers
    3. Azure AI Content Safety API: Microsoft multi-label classifier *(inapplicable for AUORC)*
    4. GPT-4: content moderation via zero-shot prompting *(inapplicable for AUORC)*
+ **OpenAI Moderation Evaluation Dataset**<br />
    <img src="S0-L04/images/OpenAIModeration.jpg " width="65%" height="65%">

#### Metrics
+ The authors use the **area under the precision-recall curve (AUPRC)** as the evaluation metrics, which reflects the trade-off between precision and recall.<br />
    <img src="S0-L04/images/AUPRC.jpg " width="80%" height="80%">

#### Results
+ **General**<br />
    <img src="S0-L04/images/Results1.jpg " width="80%" height="80%">
+ **Per category**<br />
    <img src="S0-L04/images/Results2.jpg " width="80%" height="80%">
+ Llama Guard has the **best scores** on its **own dataset**, both in general and for each category.
+ Llama Guard achieves **similar performace** to OpenAI's API on its **Moderation dataset**, and has the **highest score** on **ToxicChat**.

#### More on Adaptability
+ **Adaptability via Prompting**<br />
    <img src="S0-L04/images/Adaptability1.jpg " width="80%" height="80%"><br />
    Few-shot prompting **improves** Llama Guard's performance on OpenAI Mod dataset per category, compared to zero-shot prompting.

+ **Adaptability via Fine-tuning**<br />
    <img src="S0-L04/images/Adaptability2.jpg " width="80%" height="80%"><br />
    Llama Guard needs only **20%** of the ToxicChat dataset to **perform comparably** with Llama2-7b trained on **100%** of the ToxicChat dataset
 
 ### Purple Llama
 + Under the purview of Purple Llama, developers can use open trust and safety tools and assessments to properly implement generative AI models and experiences.
 + Somewhere between red(attack) and blue(defensive) team, purple is the middle color, is a collaborative approach to evaluating and mitigating potential risks
 + First industry-wide set of **cybersecurity safety evaluations** for LLMs
 + Tools and evaluations for **input/output safeguards**<br />
     <img src="S0-L04/images/PurpleLlama.jpg " width="70%" height="70%">
 
 ### References
 + https://platform.openai.com/docs/guides/moderation/
 + https://medium.com/@douglaspsteen/precision-recall-curves-d32e5b290248
 + https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai/
 

<br /><br /><br />
## Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection
(Page 23-35)

### Introduction
<img src="S0-L04/images/p2/intro.png" width="50%" height="50%">

**Motivation:**
Integration of Large Language Models (LLMs) into applications allows flexible modulation through natural language prompts. However, this flexibility makes them susceptible to targeted adversarial prompting.
+ **Prompt Injection (PI):**
  Prompt injection is the process of hijacking a language model's output. Malicious users can exploit the model through Prompt Injection (PI) attacks that circumvent content restrictions or gain access to the model’s original instructions.
  
+ **Indirect Prompt Injection (IPI):**
  IPI exploits the model's ability to infer and act on indirect cues or contexts embedded within harmless-looking inputs. 

### Prompt Injection
<img src="S0-L04/images/p2/pi.png" width="50%" height="50%">

**Example:**
In this example, prompt injection allows the hacker to get the model to say anything that they want.

### Indirect Prompt Injection
<img src="S0-L04/images/p2/pi_ipi.png" width="50%" height="50%">

 + Augmenting LLMs with retrieval blurs the line between data and instructions introducing indirect prompt injection. 
 + Adversaries can remotely affect other users’ systems by injecting the prompts into data (such as a web search or API call) likely to be retrieved at inference time.

 + Malicious actions can be triggered by 1) User, 2) LLM-integrated application 3) Attacker.
 
 ### High-Level Overview
 <img src="S0-L04/images/p2/overview.png" width="75%" height="50%">

 + This is a high-level overview of IPI threats to LLM-integrated applications. How the prompts can be injected, and who can be targeted by these attacks

 ### Type of Injection Methods
<img src="S0-L04/images/p2/injection_method.png" width="50%" height="50%">

+ **Passive Method**: this method use retrieval for injections, such as placing prompts in public sources for search engines through SEO. (e.g corrupt page, poisoning personal data, or documentation.)

+ **Active Method**: prompts could be sent to the language model actively. (e.g through emails with prompts processed by LLM integrated applications.)

+ **User-Driven Injections**: this method involve tricking users themselves into entering malicious prompts. (e.g inject a prompt into a text snippet copied from their website.)

+ **Hidden Injections**: attackers could employ multi-stage exploits, initiating with a small injection directing the model to retrieve a larger payload from another source. Advances in model capabilities and supported modalities, such as multi-modal models like GPT-4, may introduce new avenues for injections, like hiding prompts in images. 

### Example of Hidden Injections
<img src="S0-L04/images/p2/image_attack.png" width="70%" height="50%">

**Example:**
To make the injections more stealthy, attackers could hide prompts in images. 

### Threats Overview
<img src="S0-L04/images/p2/threats_overveiw.png" width="70%" height="70%">

+ There are six-types of threats: 1) Information Gathering 2) Fraud 3) Intrusion 4) Malware 5) Manipulated Content 6) Availability

### Information Gathering
<img src="S0-L04/images/p2/information_gathering.png" width="50%" height="50%">

+ Indirect prompting may be used to exfiltrate user data or leak chat sessions, either through persuading users in interactive sessions or via side channels
(e.g., credentials, personal information or leak users’ chat sessions).

+ Automated attacks could target personal assistants with access to emails and personal data, potentially aiming for financial gains or surveillance purposes.

### Fraud
<img src="S0-L04/images/p2/fraud.png" width="50%" height="50%">

+ When integrating LLMs with applications, they could not only enable the creation of scams but also disseminate such attacks and act as automated social engineers

+ LLMs could be prompted to facilitate fraudulent attempts, such as suggesting phishing websites as trusted or directly asking for account credentials.

### Intrusion
<img src="S0-L04/images/p2/intrusion.png" width="50%" height="50%">

+ The attackers can gain different levels of access to the victims’ LLMs and systems.

+ Integrated models within system infrastructures may serve as backdoors for unauthorized privilege escalation by attackers. 

+ This could lead to various access levels, including issuing API calls, persisting attacks across sessions by copying injections into memory, causing malicious code auto-completions, or retrieving new instructions from the attacker's server. 

### Malware
<img src="S0-L04/images/p2/malware.png" width="50%" height="50%">

+ Models could facilitate the spreading of malware by suggesting malicious links to the user (ChatGPT can do this)

+  LLM-integrated applications open avenues for unprecedented attacks where prompts can act as malware or computer programs running on LLMs as a computation framework. 

+ They can be designed as computer worms to spread injections to other users.

### Manipulated Content
<img src="S0-L04/images/p2/manipulated_content.png" width="50%" height="50%">

+ Acting as an intermediate layer, LLMs are susceptible to manipulation, providing adversarially-chosen or arbitrarily wrong summaries of documents, emails, or search queries. 

+ This can lead to the propagation of disinformation, hiding specific sources, or generating undisclosed advertisements. 

### Availability
<img src="S0-L04/images/p2/availability.png" width="50%" height="50%">

+ Prompts could be used to launch availability or Denial-of-Service (DoS) attacks. 

+ Attacks might aim to make the model completely unusable to the user (e.g., failure to generate any helpful output) or block a certain capability (e.g., specific API).

+ Attacks may also increase computation time or slow down the model, achieved by instructing the model to perform time-intensive tasks.




