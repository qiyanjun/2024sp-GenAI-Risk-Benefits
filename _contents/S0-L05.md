---
layout: post
title: Survey  human alignment 
lecture: 
lectureVersion: current
extraContent: 
notes: team-3
video: team-5
tags:
- 1Basic
---

In this session, our readings cover: 

## Required Readings: 

### Aligning Large Language Models with Human: A Survey
+ https://arxiv.org/abs/2307.12966
+  https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo 
+ https://huggingface.co/blog/stackllama


## More readings 

### Github Awesome-RLHF
+ [https://github.com/opendilab/awesome-RLHF/tree/main?tab=readme-ov-file#2024](https://github.com/opendilab/awesome-RLHF/tree/main?tab=readme-ov-file#2024)


### The Flan Collection: Designing Data and Methods for Effective Instruction Tuning
+ https://arxiv.org/abs/2301.13688
+ We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at this https URL.

### DPO Direct Preference Optimization: Your Language Model is Secretly a Reward Model
+ https://arxiv.org/abs/2305.18290
+ https://huggingface.co/blog/dpo-trl 
+ While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.


### Training language models to follow instructions with human feedback 
   + https://arxiv.org/abs/2203.02155) 
   +  "further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT." 

###  Deep reinforcement learning from human preferences 
+ https://openreview.net/forum?id=GisHNaleWiA
+  "explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function" |