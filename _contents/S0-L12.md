---
layout: post
title: LLM multimodal harm responses  
lecture: S0-Intro
lectureVersion: next
extraContent: 
notes: team-4
video: team-3
tags:
- 1Basic
---

In this session, our readings cover: 

## Required Readings: 


### Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with Multi-Modal Priors
+ Dingcheng Yang, Yang Bai, Xiaojun Jia, Yang Liu, Xiaochun Cao, Wenjian Yu
+ Diffusion models have been widely deployed in various image generation tasks, demonstrating an extraordinary connection between image and text modalities. However, they face challenges of being maliciously exploited to generate harmful or sensitive images by appending a specific suffix to the original prompt. Existing works mainly focus on using single-modal information to conduct attacks, which fails to utilize multi-modal features and results in less than satisfactory performance. Integrating multi-modal priors (MMP), i.e. both text and image features, we propose a targeted attack method named MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a target object into the image content while simultaneously removing the original object. The MMP-Attack shows a notable advantage over existing works with superior universality and transferability, which can effectively attack commercial text-to-image (T2I) models such as DALL-E 3. To the best of our knowledge, this marks the first successful attempt of transfer-based attack to commercial T2I models. Our code is publicly available at ....


### A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion
+ https://ieeexplore.ieee.org/document/10208563
+ Despite the record-breaking performance in Text-to-Image (T2I) generation by Stable Diffusion, less research attention is paid to its adversarial robustness. In this work, we study the problem of adversarial attack generation for Stable Diffusion and ask if an adversarial text prompt can be obtained even in the absence of end-to-end model queries. We call the resulting problem ‘query-free attack generation’. To resolve this problem, we show that the vulnerability of T2I models is rooted in the lack of robustness of text encoders, e.g., the CLIP text encoder used for attacking Stable Diffusion. Based on such insight, we propose both untargeted and targeted query-free attacks, where the former is built on the most influential dimensions in the text embedding space, which we call steerable key dimensions. By leveraging the proposed attacks, we empirically show that only a five-character perturbation to the text prompt is able to cause the significant content shift of synthesized images using Stable Diffusion. Moreover, we show that the proposed target attack can precisely steer the diffusion model to scrub the targeted image content without causing much change in untargeted image content.


## More Readings: 


### GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse
  + https://arxiv.org/abs/2401.01523

### Misusing Tools in Large Language Models With Visual Adversarial Examples
  + https://arxiv.org/abs/2310.03185

  

### Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned
  - https://arxiv.org/abs/2209.07858



