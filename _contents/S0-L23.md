---
layout: post
title: MultiAgent LLMs
lecture: 
lectureVersion: current
extraContent: 
notes: team-3
video: team-4
tags:
- Agent
desc: 2024-S23
term: 2024-seminarRead
categories:
- FMAdapt
---


In this session, our readings cover: 

## Required Readings: 

### Large Language Model based Multi-Agents: A Survey of Progress and Challenges
+ Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, Xiangliang Zhang
+ Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.




## More Readings: 




### Understanding the planning of LLM agents: A survey
+ https://arxiv.org/abs/2402.02716
+ As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.





### LLM Agents can Autonomously Hack Websites
+ Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang
+ In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents. In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of LLMs.



### Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models
+ Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, Feng Zhao
+ Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code will be available at this https URL.

### Humanoid Locomotion as Next Token Prediction
+ Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, Jitendra Malik
+ We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories.

### Outline
<img src="{{ site.baseurl }}/Lectures/S0-L23/images/Slide3.png" width="80%" height="80%">

The presenters identify 3 papers that each presenter will discuss:
- Large Language Model based Multi-Agents: A Survey of Progress and Challenges (presenter: Ritu)
- Understanding the Planning of LLM Agents: A Survey (presenter: Afsara)
- LLM Agents can Autonomously Hack Websites (presenter: Aidan)

<img src="{{ site.baseurl }}/Lectures/S0-L23/images/Slide4.png" width="80%" height="80%">
In this slide the presenters define what autonomous agents are, and give this equation for the formulation of the planning tasks:

$p = (a_0, a_1, ..., a_t)=plan(E, g; \Theta, P)$

Where $\Theta$ and $P$ represent the paramaters of the LLM and the parameters of the task.

<img src="{{ site.baseurl }}/Lectures/S0-L23/images/Slide5.png" width="80%" height="80%">
The presentors now discuss conventional approaches to autonomous agents, specifically bringing up the jugs-and-water problem. 

In terms of the symbolic methods, they describe the Planning Domain Definition Language (PDDL), which may require the efforts of human experts and lacks error tolerance. 

Policy learning is a reinforcement learning based method, which requires many data points and thus can be impractical if data collection is expensive.

<img src="{{ site.baseurl }}/Lectures/S0-L23/images/Slide6.png" width="80%" height="80%">
The presenters show a graphic from the authors which is their "novel and systematic" taxonomy for LLM planning, and which defines existing tools into 5 key categories: External Planner, Reflection, Memory, Decomposition, and Selection.

<img src="{{ site.baseurl }}/Lectures/S0-L23/images/Slide7.png" width="80%" height="80%">
This table provides more information on the taxonomy categories from the previous slide, and shows some specific representative works for each. 

<img src="{{ site.baseurl }}/Lectures/S0-L23/images/Slide8.png" width="80%" height="80%">
The authors discuss task decomposition, and specifically decomposition-first methods, wherein a model decomposes a task into subgoals which are planned for. This reduces the risk of hallucination and forgetting, but also has an additional requirement for adjustment mechanisms.

<img src="{{ site.baseurl }}/Lectures/S0-L23/images/Slide9.png" width="80%" height="80%">
In interleaved task decomposition, decomposition is dynamically adjusted based on environmental feedback. This improves fault tolerance. However, this can lead to hallucinations for very complicated tasks. Planning can also be constrained by the context length of the LLM.

<img src="{{ site.baseurl }}/Lectures/S0-L23/images/Slide10.png" width="80%" height="80%">
In multi-plan selection, an agent generates multiple plans and selects the optimal one. In self-consistency, multiple distinct reasoning paths are used and the naive majority vote strategy is used to pick the optimal one. 

In Tree-of-Thought, two explicit strategies, "sample" and "propose", are used to generate and sample plans in the reasoning process. This process supports tree-search algorithms like BFS and DFS. 

<img src="{{ site.baseurl }}/Lectures/S0-L23/images/Slide11.png" width="80%" height="80%">
The presenters show a visualization of the previously discussed methods as well as Input-Output Prompting and Self-Consistency with CoT. 

<img src="{{ site.baseurl }}/Lectures/S0-L23/images/Slide12.png" width="80%" height="80%">
This slide shows how an agent might answer questions related to thought generation and valuation of plans for the Game of 24, a mathematical reasoning challenge where the goal is to use 4 numbers and arithmetic operations to obtain 24. 