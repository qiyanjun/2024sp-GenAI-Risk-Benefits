---
layout: post
title: Advanced Transformer Architectures
lecture: W14_LLM_advanced_arch
lectureVersion: current 
extraContent: 
notes: team-6
video: team-6
tags:
- Efficiency
desc: 2024-S26
term: 2024-seminarRead
categories:
- FMEfficient
---



In this session, our readings cover: 

## Required Readings: 



### Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey
+ https://arxiv.org/abs/2311.12351
+ Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research. A curated repository of relevant literature, continuously updated, is available at this https URL.





### FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
+ Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher R√©
+ Paper: https://arxiv.org/abs/2205.14135
+ Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).

+ Related: [blogpost FlashAttention
 ‚Äî Techniques for Efficient Inference 
 of LLMs (III/IV)](https://medium.com/mantisnlp/flashattention-techniques-for-efficient-inference-of-llms-iii-iv-0d619c9ca38c)


### JAMBA
+ Introducing Jamba: AI21's Groundbreaking SSM-Transformer Model
Debuting the first production-grade Mamba-based model delivering best-in-class quality and performance.
+ March 28, 2024
+ https://www.ai21.com/blog/announcing-jamba
+ We are thrilled to announce Jamba, the world‚Äôs first production-grade Mamba based model. By enhancing Mamba Structured State Space model (SSM) technology with elements of the traditional Transformer architecture, Jamba compensates for the inherent limitations of a pure SSM model. Offering a 256K context window, it is already demonstrating remarkable gains in throughput and efficiency‚Äîjust the beginning of what can be possible with this innovative hybrid architecture. Notably, Jamba outperforms or matches other state-of-the-art models in its size class on a wide range of benchmarks.



## More readings: 

### Mamba: Linear-Time Sequence Modeling with Selective State Spaces
+ Albert Gu, Tri Dao
+ https://arxiv.org/abs/2312.00752
+ Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5√ó higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.




### Efficient Memory Management for Large Language Model Serving with PagedAttention
+ Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica
+ High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4√ó with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at this https URL





### Attention Mechanisms in Computer Vision: A Survey
+ Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai Zhang, Ralph R. Martin, Ming-Ming Cheng, Shi-Min Hu
+ https://arxiv.org/abs/2111.07624
+ Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention mechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention mechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have achieved great success in many visual tasks, including image classification, object detection, semantic segmentation, video understanding, image generation, 3D vision, multi-modal tasks and self-supervised learning. In this survey, we provide a comprehensive review of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial attention, temporal attention and branch attention; a related repository this https URL is dedicated to collecting related work. We also suggest future directions for attention mechanism research.

# State Space Model for New-Generation Network Alternative to Transformers: A Survey
## Motivation
**Pros and Cons of Attention**
- Self-attention mechanism has successfully enabled transformer to learn long-range feature representations well.
- However, Transformer-based models require high-end GPU with larger memory for training and testing/deployment.

Hence, We need a model that not only requires **less computing cost** but also is still able to capture **long-range dependencies** while maintaining high performance.

That's what **State Space Model** (SSM) wants to solve.

## Formulation of SSM
SSM is a commonly used model in control theory and is used in Kalman simulation and hidden Markov models. Its basic formulation is shown in the figure below.

<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/01_SSM_Formulation.png" width="80%" height="80%"></p>

Normally, we would omit the parameter **D** (assume **D**=0 becuase the term **D**u can be viewed as a skip connection and is easy to compute). So a more common formulation we would see in most state space model would be as:
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/02_SSM_wo_D.png" width="50%" height="50%"></p>

## Discretization
As a continuous system, it is hard for SSM to be used in modern deep learning algorithm. In practice, we always deal with discrete data, such as text. This requires us to discretize the SSM, transforming our continuous parameters **A**, **B**, **C** into discrete parameters $\hat{A}, \hat{B}$ using zero-order hold rule (ZOH) as shown in Figure below. Readers can refer to the paper for detailed derivation.
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/03_Discretization.png" width="50%" height="50%"></p>

In conclusion, the discretized version of SSM is like:
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/04_Discretization_Formulation.png" width="30%" height="30%"></p>

## Convolutional Form
Unlike RNN, SSM here doesn't have non-linear functions. So we can try to expand $y_t$ and surprisingly find SSM can be written in convolutional form. 
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/05_yt_Expansion.png" width="30%" height="30%"></p>

Looking at the result of the expansion above, we can see that the coefficient of each $x_t$ can be extracted out and write a convolutional kernel:
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/06_Convolutional_Kernel.png" width="30%" height="30%"></p>

Hence, we can write our SSM formulation as:
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/07_Convolutional_Form.png" width="30%" height="30%"></p>

It's easy to find that SSM is very similar to RNN. Comparing the formulation of SSM and RNN below, we can find the main reason why RNN can't be written in convolutional form and thus can't be trained efficiently is the non-linear funciton $f$.
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/08_SSM_vs_RNN.png" width="30%" height="30%"></p>

## Structured State Space Model (S4)

- Similar to RNNs, SSM also suffers from the vanishing/exploding gradients problem when modeling longer sequences.

To solve this problem, HiPPO matrices is introduced which combines the concepts of Recurrent Memory and Optimal Polynomial Projections, thus can significantly improve the performance of recursive memory. 

In practice, we would use HiPPO matrix to initial like matrix A.
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/09_HiPPO_Matrix.png" width="30%" height="30%"></p>

Note the **"Structured"** comes from the HiPPO matrix. And we usually can the vallila SSM with HiPPO matrix :**S4 model** in short which will be seen in most SSM related papers.

## From S4 to Mamba (S6)
The problem of S4:
- S4 does not have **selectivity**
- Those discrete parameters are constant
Those problem will result in the S4 treat all part of the input exactly the same like the Figure shown below.
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/10_No_Selectivity.png" width="80%" height="80%"></p>

Mamba makes these parameters vary based on the input, like the formulation below:
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/11_Mamba_Formulation.png" width="50%" height="50%"></p>
By doing so, model has the ability to focus on certain words, like the Figure shown below.
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/12_Mamba_Result.gif" width="80%" height="80%"></p>

## Parallization of Mamba
- In S4, we are able to precompute this kernel, save it, and multiply it with the input x.
- However, in Mamba, these matrices change depending on the input.
- If we want selectivity, we‚Äôll need to train Mamba with RNN mode.

Mamba is able to solve this problem throught **parallel scan**.

**Parallel Scan**
Whether an operation can be done in parallel depends on **associative property**. Mamba's recurrence was very similar to a scan algorithm, also known as a prefix sum.
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/13_Mamba_Recurrence.png" width="50%" height="50%"></p>
We can verify its associative property with a new variable k:
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/14_Verify.png" width="50%" height="50%"></p>
Figure below shows how parallel scan works. We can pick any vertical line and start from the top of this line and move to the bottom, tracing each addition back to the array‚Äôs first few items. By the time we reach the bottom, we should have the sum of all items to the left of this line.
<p align="center"><img src="{{ site.baseurl }}/Lectures/S0-L26/images/SSM_Survey/15_Parallel_Scan.png" width="80%" height="80%"></p>

# Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey

## **Introduction to Long-Context LLMs**

- Great Success for Transformer-based LLM Models (chatGPT, Bert, Claude..)
  - Indicates a potential path towards AGI
  - Revolutionizing Application: Document summarization, Computer vision, ‚Ä¶
  - Essential for advanced applications
    - like detailed text analysis and interactive AI systems

- Success due to well-designed **Attention Mechanism,** but ‚Ä¶

### **Challenges and Research Directions in Long-Context LLMs**

- **Challenges in Current Transformer Models**
  - **Complexities**: High computational needs with quadratic time and space complexities during training and inference
    - **Performance Degradation**: Lack of robustness in mechanism leads to **performance degradation** with long sequences

- **Research Directions**
  - **Efficiency Improvements**: Attention mechanism, memory mechanisms
  - **Handling Long Contexts**: Effective length generalization, context pre/post processing

### **Contributions of this Survey**

- **Holistic Taxonomy**:Detailed breakdown of Transformer architecture enhancements
- **Evaluations and Toolkits**: Analysis of datasets, metrics, libraries, frameworks for optimizing LLM efficiency
- **Future Directions**: Identifying key challenges and potential solutions for advancing long-context comprehension in LLMs.

## **Section 2: Overview** 

### **Preliminaries of Neural Language Modeling**

- **Modeling Stages**
  - **Preprocessing**: Tokenization of raw text into subwords or tokens
  - **Pretraining**: Learning semantic patterns and linguistic structures on large corpora
  - **Fine-tuning**: Adapting the pre-trained model to task-specific data for downstream applications
  - **Inference**: Auto regressively generating text based on learned probabilities
- **Key-Value Cache in LLMs**
  - **Functionality**: Stores key-value pairs for attention, extending sequences during generation
  - **Limitation**: Linearly growing memory occupation with generated tokens, prompting long-term memory enhancements

### **Limitations of Transformer Architecture in Handling Long Contexts**

- **Attention Complexity**
  - Computational Complexity: In scenarios where **sequence length** ùêø far exceeds **dimension** ùëë
    -  The complexity becomes quadratic
    - Time Complexity: ùëÇ(ùêø^2*d) Space Complexity: ùëÇ(ùêø^2) 
- **In-context Memory Limitations** 
  - **Statelessness of Transformers**: **Lacks a mechanism to retain state** between calls, relying only on the KV cache
  - **Impact on Applications**: This design limits effectiveness in applications requiring long-term memory(chatbots)
- **Max-Length Constraint**
  - **Training and Inference**: **Engineers set a maximum sequence length** ùêøùëöùëéùë• to prevent memory overflow
    - As a hyper-parameter, typically between 1K, 2K 4K tokens
  - **Performance degradation**: observed when handling inputs longer than ùêøùëöùëéùë• resulting in **implausible outputs**

### Roadmap of Enhancements for Long-Context Capabilities in LLMs

<img src="{{ site.baseurl }}/Lectures/S0-L26/images/long-context-figure/1.jpg" width="100%" height="100%">



## **Section 3: Efficient Attention Mechanisms**

- **Goal**: Addressing the **computational bottleneck of attention mechanisms** in Transformers
- **Impact**: **Expanding the context length boundary** for LLMs during both pre training and inference phases
- **Category**
  - **Local Attention**
  - **Hierarchical Attention**
  - **Sparse Attention**
  - **Approximated Attention**
  - **IO-Aware Attention**

### **Local Attention**

- **Redefining Attention Mechanisms**
  - **Traditional Global Attention**: Each token attends to all others, leading to ùëÇ(ùêø^2ùëë) complexities
  - **Local Attention**: Focuses on neighboring tokens, reducing time and space complexities
- **Approaches**
  - **Block-wise Attention**
    - Divides input into non-overlapping blocks, each attending within itself(e.g. BlockBERT)
  - **Sliding Window Attention**
    - Each token attends within a fixed-size window, inspired by CNN techniques(e.g. Longformer)
  - **Global-Local Hybrid Attention**
    - Combines local attention **with global tokens** for broader context (e.g. LongLM)
  - **LSH Attention**
    - Utilizes **locality-sensitive hashing** for efficient neighbor token selection

<img src="{{ site.baseurl }}/Lectures/S0-L26/images/long-context-figure/2.jpg" width="100%" height="100%">



### **Hierarchical Attention**

- **Goal:** Merge higher-level global information with lower-level local attention for efficient and scalable processing
- **Impact**
  -  **Complexity Reduction**: Achieves sub-quadratic computational and memory costs while preserving the expressiveness of full attention
  - **Contextual Balance**: Maintains a balance between local and global context for inherent locality principle
- **Approaches**
  - **Two-Level Hierarchy**
    - Uses self-attention across two levels: word-to-sentence and sentence-to-document (e.g. HAN)
  - **Multi-Level Hierarchy**
    - **Introduces fine-to-coarse attention via **binary partitioning**, formalizing as a graph neural network(e.g BPT)
    - Controls attention span with a soft attention mask (e.g. Adaptive Span Transformer)
  - **Advanced Hierarchical Mechanisms**
    - Partitions attention matrix into blocks with different low-rank ranges (e.g. H-Transformer-1D)
    - Combines full-attention approximation with structured factorization (e.g. Combiner) 

<img src="{{ site.baseurl }}/Lectures/S0-L26/images/long-context-figure/3.jpg" width="100%" height="100%">


### **Approximated Attention**

- **Goal**: Reduce the full attention computation by **leveraging sparsity and low-rankness** with linear complexity, optimizing precision trade-offs
- **Impact**: Provides sub-quadratic computation and memory complexity while maintaining the expressiveness of full attention
- **Techniques**
  - **Low-Rank Approximation**
    - Linformer: Utilizes SVD for a low-rank approximation of the attention matrix, reducing complexity to ùëÇ(ùêøùëòùëë)
  - **Nested Attention**
    - Luna: Combines pack and unpack attention strategies to handle sequences of varying lengths without compromising parallelism
  - **Kernelized Approximation**
    - Linear Transformer & Performer: Introduces **kernel-based attention** approximations, significantly cutting down on computational resources
  - **Hybrid Approaches**
    - **Sparse-Kernelized Hybrid**
    - **Scatterbrain**: combines **sparse matrices and kernelized feature maps** for enhanced efficiency and precision



### **IO-Aware Attention**

- **Different**
  - Previous attention methods **trade off some attention quality for lower computation**
  - But IO-aware methods maintain exactness of attention calculations while **optimizing computational resources**
- **Offer exact attention computations with significantly reduced memory and time consumption**A leap forward in the optimization of Transformer models for large-scale applications
- **Techniques**
  - **Memory-Efficient Attention:** Utilizes lazy softmax algorithm for numerically stable attention
  - **Flash Attention**: Achieves up to **7.6x speedup and 20x memory efficiency** with exact attention computation
  - **Paged Attention**Addresses inference memory bottlenecks by **managing KV cache memory** with virtual memory paging techniques, improving efficiency and flexibility for batched requests
  - **Innovations and Improvements**Sparse Clustered Factorization Attention: Extends Flash Attention to accommodate diverse sparsity patterns, **leading to 2 to 3.3 times training speedup**
  - Virtual Large Language Models: Proposes techniques to manage growing KV cache memory 



## **Section 4: Long-Term Memory**
